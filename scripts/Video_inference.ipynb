{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486b70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import shutil\n",
    "import inspect\n",
    "from collections import OrderedDict,defaultdict\n",
    "import yaml\n",
    "import time\n",
    "from feeders import tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2576056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(bbox):\n",
    "    xc,yc,w,h = bbox\n",
    "    x1 = xc - (w/2)\n",
    "    y1 = yc - (h/2)\n",
    "    x2 = xc + (w/2)\n",
    "    y2 = yc + (h/2)\n",
    "    return [x1,y1,x2,y2]\n",
    "\n",
    "def xyxy2xywh(bbox):\n",
    "    x1,y1,x2,y2 = bbox\n",
    "    xc = (x1+x2)/2\n",
    "    yc = (y1+y2)/2\n",
    "    w = x2-x1\n",
    "    h = y2-y1\n",
    "    return [xc,yc,w,h]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628f1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseTrackV8(object):\n",
    "    def __init__(self,weightPath = '/home/k100/Code/weights/yolov8l-pose.pt',\n",
    "                      device = 'cuda:0'):\n",
    "        self.model = YOLO(weightPath)\n",
    "        self.device = device\n",
    "    def poseTrack(self, imgs,device=None):\n",
    "        return self.model.track(imgs,\n",
    "                                stream=False,\n",
    "                                device=self.device,\n",
    "                                tracker=\"bytetrack.yaml\",\n",
    "                                persist=True,\n",
    "                                verbose=False) \n",
    "class KptTrack(object):\n",
    "    def __init__(self,gg=None):\n",
    "        self.gg = gg\n",
    "        self.track_history = defaultdict(lambda: []) \n",
    "        self.drop_counting = defaultdict(lambda: 0) \n",
    "        self.max_miss = 500\n",
    "    def tracking(self,trackIDs,kpts):\n",
    "        diff = list(set(list(set(self.track_history.keys()))).difference(trackIDs))\n",
    "        for d in diff:\n",
    "            if self.drop_counting[d] > self.max_miss:\n",
    "                del self.drop_counting[d]\n",
    "                del self.track_history[d]\n",
    "            else:\n",
    "                self.drop_counting[d]+=1\n",
    "        trackIDsNew = [] ; kptSeqs = []\n",
    "        for trackID,kpt in zip(trackIDs,kpts):\n",
    "            track = self.track_history[trackID]\n",
    "            track.append(kpt)\n",
    "            trackIDsNew.append(trackID)\n",
    "            kptSeqs.append(torch.stack(track))\n",
    "        return trackIDsNew,kptSeqs\n",
    "\n",
    "    def sameShaper(self,kptSeqsList):\n",
    "        if isinstance(kptSeqsList[0],torch.Tensor):\n",
    "            kptSeqsList = [kptSeq.numpy() for kptSeq in kptSeqsList]\n",
    "        if len(kptSeqsList) == 1:\n",
    "            return np.array(kptSeqsList)\n",
    "        min_len = np.min([len(kptSeq) for kptSeq in kptSeqsList])\n",
    "        # for kptSeq in kptSeqsList:\n",
    "            # print(kptSeq.shape,min_len) \n",
    "        return np.array([kptSeq[-min_len:] for kptSeq in kptSeqsList])\n",
    "\n",
    "    def getBBOX_from_kpt(self,kpt,outsizeW=2,outsizeH=1.5):\n",
    "        kptNp = kpt.copy()#.numpy()\n",
    "        mask = kptNp[..., 2] < 0.5\n",
    "        min_xy = kptNp[..., :2].max(axis=(0, 1))\n",
    "\n",
    "        kptNp[mask, :2] = min_xy\n",
    "        x1 = np.min(kptNp[:,:,0])\n",
    "        y1 = np.min(kptNp[:,:,1])\n",
    "        x2 = np.max(kptNp[:,:,0])\n",
    "        y2 = np.max(kptNp[:,:,1])\n",
    "        # print(x1,y1,x2,y2)\n",
    "        xc,yc,w,h = xyxy2xywh([x1,y1,x2,y2])\n",
    "        w *= outsizeW\n",
    "        h *= outsizeH\n",
    "        x1,y1,x2,y2 = xywh2xyxy([xc,yc,w,h])\n",
    "        bbox = np.array([x1,y1,x2,y2]).astype(int)\n",
    "        bbox[bbox<0] = 0\n",
    "        return bbox\n",
    "        \n",
    "class MotionRecognizeSkateFormerRGB(object):\n",
    "    def __init__(self,ckptPath = './work_dir/ntu/cs/SkateFormerRGB_j/runs-last_model_Epoch90_acc98.pt',\n",
    "                      modelType = 'model.SkateFormer.SkateFormerRGB_',\n",
    "                      device = 'cuda:0',\n",
    "                      clsNamePath = './nturgbd120_cls.txt'\n",
    "                ):\n",
    "        \n",
    "        self.model_args = {'num_classes': 120, 'num_people': 2, 'num_points': 17,\n",
    "                      'kernel_size': 7, 'num_heads': 32, 'attn_drop': 0.5,\n",
    "                      'head_drop': 0.0, 'rel': True, 'drop_path': 0.2, \n",
    "                      'type_1_size': [8, 2], 'type_2_size': [8, 17],\n",
    "                      'type_3_size': [8, 2], 'type_4_size': [8, 17],\n",
    "                      'mlp_ratio': 4.0, 'index_t': True}\n",
    "        \n",
    "        self.ckptPath = ckptPath\n",
    "        self.modelType = modelType \n",
    "        self.device = device\n",
    "        self.load_model()\n",
    "        self.maxPersonNum = 2\n",
    "        self.kptNum = 17\n",
    "        self.maxFrameNum = 100\n",
    "        self.dim = 3\n",
    "        ## (batch, 100, 102)\n",
    "        self.p_interval = [0.95]\n",
    "        self.window_size = 64\n",
    "        self.thres = 64\n",
    "        self.clsNamePath = clsNamePath\n",
    "        self.load_clsNameTxt()\n",
    "    def load_clsNameTxt(self):\n",
    "        if os.path.exists(self.clsNamePath):\n",
    "            with open(self.clsNamePath,'r') as f:\n",
    "                self.clsList = [line[:-1] for line in f.readlines()]\n",
    "        else:\n",
    "            print(f'clsNamePath ({self.clsNamePath}) is not exists so skip')\n",
    "            self.clsList = None\n",
    "    def cropPreprocess(self,crop):\n",
    "        cropH,cropW,_ = crop.shape\n",
    "        assert cropH==256 and cropW==128\n",
    "        crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB).astype(float)/255\n",
    "        return torch.from_numpy(np.transpose(crop, (2, 0, 1)))#.unsqueeze(0)\n",
    "\n",
    "    def import_class(self,import_str):\n",
    "        mod_str, _sep, class_str = import_str.rpartition('.')\n",
    "        __import__(mod_str)\n",
    "        try:\n",
    "            return getattr(sys.modules[mod_str], class_str)\n",
    "        except AttributeError:\n",
    "            raise ImportError('Class %s cannot be found (%s)' % (class_str, traceback.format_exception(*sys.exc_info())))\n",
    "\n",
    "    def load_model(self):\n",
    "        print('[modelType] : ', self.modelType)\n",
    "        Model = self.import_class(self.modelType)\n",
    "        self.model = Model(**self.model_args)\n",
    "        weights = torch.load(self.ckptPath)\n",
    "        weights = OrderedDict([[k.split('module.')[-1], v.to(self.device)] for k, v in weights.items()])\n",
    "        keys = list(weights.keys())\n",
    "        self.model.load_state_dict(weights)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def normalization(self,data,frameShape=(1080,1920)):\n",
    "        height,width = frameShape\n",
    "        data[...,0]/=width\n",
    "        data[...,1]/=height\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def kptsPreprocess(self,kptSeqs,img):\n",
    "        imgSacle = img.shape[:-1]\n",
    "        kpts = [kptSeq[-1,:,:] for kptSeq in kptSeqs]\n",
    "        bboxs = [KptTracker.getBBOX_from_kpt(kpt[:,:,:].numpy()) for kpt in kpts]\n",
    "        # print('bbox len : ', len(bboxs))\n",
    "        crops = torch.stack([self.cropPreprocess(cv2.resize(img[int(y1):int(y2),int(x1):int(x2)],(128,256))) \n",
    "                             for x1,y1,x2,y2 in bboxs],dim=0)\n",
    "        \n",
    "        kptsN = [self.normalization(kptSeq.reshape(kptSeq.shape[1],51*kptSeq.shape[0]),frameShape=imgSacle) for kptSeq in kptSeqs]\n",
    "        kptsNN = np.zeros((len(kptsN), self.maxFrameNum, self.kptNum*self.maxPersonNum*self.dim), dtype=np.float32)\n",
    "        for dIdx,kptN in enumerate(kptsN):\n",
    "            if kptN.shape[-2] > self.maxFrameNum:\n",
    "                kptN = kptN[-self.maxFrameNum,:]\n",
    "            kptsNN[dIdx, :kptN.shape[0],:kptN.shape[-1]] = kptN\n",
    "        return kptsNN,crops,bboxs\n",
    "\n",
    "    def inference(self,Inputdata,Inputcrops):\n",
    "        inputdata = Inputdata.reshape((len(Inputcrops), self.maxFrameNum, 2, 17, 3)).transpose(0, 4, 1, 3, 2)\n",
    "        t_indexs = np.empty((len(inputdata),64))#,dtype=float\n",
    "        datas = np.empty((len(inputdata),3,64,17,2))#,dtype=float\n",
    "        t_indexs = np.empty((len(inputdata),64))#,dtype=float\n",
    "        valid_frame_nums = np.sum(np.sum(np.sum(inputdata, axis=-1), axis=-1) != 0, axis=-1)[...,0]\n",
    "\n",
    "        for i in range(len(inputdata)):\n",
    "            data,t_index = tools.valid_crop_uniform(inputdata[i], \n",
    "                                                    valid_frame_nums[i], \n",
    "                                                    self.p_interval, \n",
    "                                                    self.window_size, \n",
    "                                                    self.thres)\n",
    "            datas[i] = data\n",
    "            t_indexs[i] = t_index\n",
    "        datas = torch.from_numpy(datas).float().to(self.device)\n",
    "        t_indexs = torch.from_numpy(t_indexs).float().to(self.device)\n",
    "        Inputcrops = Inputcrops.float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(datas,t_indexs,Inputcrops)\n",
    "        preds = torch.nn.functional.softmax(preds, dim=0)\n",
    "        scores = torch.max(preds,dim=1).values.cpu()\n",
    "        prs = torch.argmax(preds,dim=1).cpu()\n",
    "\n",
    "        return prs,scores\n",
    "        \n",
    "    def drawBoundingBox(self,img, bboxs, actIs, actSs,fontScale=1,thickness=2):\n",
    "        for box,actI,actS in zip(bboxs,actIs,actSs):\n",
    "            x1,y1,x2,y2 = box[:4]\n",
    "            actI = self.clsList[actI] if self.clsList is not None else actI\n",
    "            actS = int(actS*100)\n",
    "            label = f'{actI}({actS}%)'\n",
    "            cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 6)\n",
    "            fontFace = cv2.FONT_HERSHEY_COMPLEX\n",
    "            labelSize = cv2.getTextSize(label, fontFace, fontScale, thickness)\n",
    "            _x1 = x1 # bottomleft x of text\n",
    "            _y1 = y1 # bottomleft y of text\n",
    "            _x2 = x1+labelSize[0][0] # topright x of text\n",
    "            _y2 = y1-labelSize[0][1] # topright y of text\n",
    "            cv2.rectangle(img, (_x1,_y1), (_x2,_y2), (0,255,0), cv2.FILLED) # text background\n",
    "            cv2.putText(img, label, (x1,y1), fontFace, fontScale, (0,0,0), thickness)\n",
    "        return img\n",
    "        \n",
    "        \n",
    "    def drawRecongizeResult(self, img, bboxs, actIs, actSs,fontScale=1,thickness=2):\n",
    "        return self.drawBoundingBox(img.copy(),bboxs,actIs, actSs,fontScale=1,thickness=2)\n",
    "        \n",
    "class VideoCaptureSave(object):\n",
    "    def __init__(self):\n",
    "        self.img_list = []\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4 codec\n",
    "        self.fps = 30  # Default frames per second\n",
    "        self.frame_width = None\n",
    "        self.frame_height = None\n",
    "        self.video_writer = None\n",
    "\n",
    "    def init_parameter(self, frame_width, frame_height, output_file, fps=30):\n",
    "        \"\"\"Initialize parameters and video capture.\"\"\"\n",
    "        self.fps = fps\n",
    "        \n",
    "        # Get frame dimensions\n",
    "        self.frame_width = int(frame_width)\n",
    "        self.frame_height = int(frame_height)\n",
    "        self.output_file = output_file\n",
    "        # Initialize video writer\n",
    "        self.video_writer = cv2.VideoWriter(output_file, self.fourcc, self.fps, (self.frame_width, self.frame_height))\n",
    "\n",
    "    def addImg2list(self,img):\n",
    "        #self.img_list.append(frame)\n",
    "        self.img_list.append(cv2.resize(img,(self.frame_width,self.frame_height)))\n",
    "\n",
    "    def saveImgList2mp4(self):\n",
    "        \"\"\"Save the images in the list to an MP4 file.\"\"\"\n",
    "        print(f'ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ {len(self.img_list)}')\n",
    "        for img in self.img_list:\n",
    "            self.video_writer.write(cv2.resize(img,(self.frame_width,self.frame_height)))\n",
    "\n",
    "        # Release the video writer\n",
    "        self.video_writer.release()\n",
    "        print(f\"Video saved successfully.  ---> {self.output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b52efd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modelType] :  model.SkateFormer.SkateFormerRGB_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k100/anaconda3/envs/motion/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "MotionRecognizeSkateFormerRGBer = MotionRecognizeSkateFormerRGB()\n",
    "KptTracker = KptTrack()\n",
    "PoseTracker = PoseTrackV8()\n",
    "VideoCaptureSaver = VideoCaptureSave()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b1712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = './samples/ntu_sample.avi'\n",
    "# videoPath = './samples/vlc-record-2024-04-24-15h03m46s-0x645A1775_20230509183809_20230509185044.avi-.mp4'\n",
    "cap = cv2.VideoCapture(videoPath)\n",
    "\n",
    "outs = []\n",
    "count = 0\n",
    "while True:\n",
    "    ret,img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    out = PoseTracker.poseTrack(img)[0]\n",
    "    outs.append(out)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797fa737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:02<00:00, 33.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ 72\n",
      "Video saved successfully.  ---> out.mp4\n"
     ]
    }
   ],
   "source": [
    "VideoCaptureSaver.init_parameter(960, 540, 'out.mp4')\n",
    "for out in tqdm(outs):\n",
    "    img = out.orig_img\n",
    "    imgH,imgW,_ = img.shape\n",
    "    trackIDs = out.boxes.id\n",
    "    trackIDs = trackIDs.cpu().numpy() if trackIDs is not None else np.array([])\n",
    "    keypoints = out.keypoints.data.cpu() if len(trackIDs) else torch.tensor([])\n",
    "    # keypoints[...,0]*=(1920/imgW)\n",
    "    # keypoints[...,1]*=(1080/imgH)\n",
    "    trackIDs,kptSeqs = KptTracker.tracking(trackIDs,keypoints)\n",
    "    kptsFeatures,crops,bboxes = MotionRecognizeSkateFormerRGBer.kptsPreprocess([kptSeq.unsqueeze(0) \n",
    "                                                                         for kptSeq in kptSeqs],img)\n",
    "    preds,scores = MotionRecognizeSkateFormerRGBer.inference(kptsFeatures,crops)\n",
    "    img_plot = MotionRecognizeSkateFormerRGBer.drawRecongizeResult(img,bboxes,preds,scores,\n",
    "                                                                   fontScale=1*(imgW/1920),thickness=int(2*(imgW/1920)))\n",
    "    VideoCaptureSaver.addImg2list(img_plot)\n",
    "VideoCaptureSaver.saveImgList2mp4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350d4b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = PoseTracker.poseTrack('sample.png')[0]\n",
    "h,w,_ = result.orig_img.shape\n",
    "result.orig_img = np.ones((h,w,3),dtype='uint8')*255\n",
    "cv2.imwrite('sample_pose.png',result.plot(labels=False,boxes=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011c04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
