{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486b70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import shutil\n",
    "import inspect\n",
    "from collections import OrderedDict,defaultdict\n",
    "import yaml\n",
    "import time\n",
    "from feeders import tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2576056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(bbox):\n",
    "    xc,yc,w,h = bbox\n",
    "    x1 = xc - (w/2)\n",
    "    y1 = yc - (h/2)\n",
    "    x2 = xc + (w/2)\n",
    "    y2 = yc + (h/2)\n",
    "    return [x1,y1,x2,y2]\n",
    "\n",
    "def xyxy2xywh(bbox):\n",
    "    x1,y1,x2,y2 = bbox\n",
    "    xc = (x1+x2)/2\n",
    "    yc = (y1+y2)/2\n",
    "    w = x2-x1\n",
    "    h = y2-y1\n",
    "    return [xc,yc,w,h]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628f1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseTrackV8(object):\n",
    "    def __init__(self,weightPath = '/home/k100/Code/weights/yolov8l-pose.pt',\n",
    "                      device = 'cuda:0'):\n",
    "        self.model = YOLO(weightPath)\n",
    "        self.device = device\n",
    "    def poseTrack(self, imgs,device=None):\n",
    "        return self.model.track(imgs,\n",
    "                                stream=False,\n",
    "                                device=self.device,\n",
    "                                tracker=\"bytetrack.yaml\",\n",
    "                                persist=True,\n",
    "                                verbose=False) \n",
    "class KptTrack(object):\n",
    "    def __init__(self,gg=None):\n",
    "        self.gg = gg\n",
    "        self.track_history = defaultdict(lambda: []) \n",
    "        self.drop_counting = defaultdict(lambda: 0) \n",
    "        self.max_miss = 500\n",
    "    def tracking(self,trackIDs,kpts):\n",
    "        diff = list(set(list(set(self.track_history.keys()))).difference(trackIDs))\n",
    "        for d in diff:\n",
    "            if self.drop_counting[d] > self.max_miss:\n",
    "                del self.drop_counting[d]\n",
    "                del self.track_history[d]\n",
    "            else:\n",
    "                self.drop_counting[d]+=1\n",
    "        trackIDsNew = [] ; kptSeqs = []\n",
    "        for trackID,kpt in zip(trackIDs,kpts):\n",
    "            track = self.track_history[trackID]\n",
    "            track.append(kpt)\n",
    "            trackIDsNew.append(trackID)\n",
    "            kptSeqs.append(torch.stack(track))\n",
    "        return trackIDsNew,kptSeqs\n",
    "\n",
    "    def sameShaper(self,kptSeqsList):\n",
    "        if isinstance(kptSeqsList[0],torch.Tensor):\n",
    "            kptSeqsList = [kptSeq.numpy() for kptSeq in kptSeqsList]\n",
    "        if len(kptSeqsList) == 1:\n",
    "            return np.array(kptSeqsList)\n",
    "        min_len = np.min([len(kptSeq) for kptSeq in kptSeqsList])\n",
    "        # for kptSeq in kptSeqsList:\n",
    "            # print(kptSeq.shape,min_len) \n",
    "        return np.array([kptSeq[-min_len:] for kptSeq in kptSeqsList])\n",
    "\n",
    "    def getBBOX_from_kpt(self,kpt,outsizeW=2,outsizeH=1.5):\n",
    "        kptNp = kpt.copy()#.numpy()\n",
    "        mask = kptNp[..., 2] < 0.5\n",
    "        min_xy = kptNp[..., :2].max(axis=(0, 1))\n",
    "\n",
    "        kptNp[mask, :2] = min_xy\n",
    "        x1 = np.min(kptNp[:,:,0])\n",
    "        y1 = np.min(kptNp[:,:,1])\n",
    "        x2 = np.max(kptNp[:,:,0])\n",
    "        y2 = np.max(kptNp[:,:,1])\n",
    "        # print(x1,y1,x2,y2)\n",
    "        xc,yc,w,h = xyxy2xywh([x1,y1,x2,y2])\n",
    "        w *= outsizeW\n",
    "        h *= outsizeH\n",
    "        x1,y1,x2,y2 = xywh2xyxy([xc,yc,w,h])\n",
    "        bbox = np.array([x1,y1,x2,y2]).astype(int)\n",
    "        bbox[bbox<0] = 0\n",
    "        return bbox\n",
    "        \n",
    "class MotionRecognizeSkateFormerRGB(object):\n",
    "    def __init__(self,ckptPath = './work_dir/ntu/cs/SkateFormerRGB_j/runs-last_model_Epoch90_acc98.pt',\n",
    "                      modelType = 'model.SkateFormer.SkateFormerRGB_',\n",
    "                      device = 'cuda:0',\n",
    "                      clsNamePath = './nturgbd120_cls.txt'\n",
    "                ):\n",
    "        \n",
    "        self.model_args = {'num_classes': 120, 'num_people': 2, 'num_points': 17,\n",
    "                      'kernel_size': 7, 'num_heads': 32, 'attn_drop': 0.5,\n",
    "                      'head_drop': 0.0, 'rel': True, 'drop_path': 0.2, \n",
    "                      'type_1_size': [8, 2], 'type_2_size': [8, 17],\n",
    "                      'type_3_size': [8, 2], 'type_4_size': [8, 17],\n",
    "                      'mlp_ratio': 4.0, 'index_t': True}\n",
    "        \n",
    "        self.ckptPath = ckptPath\n",
    "        self.modelType = modelType \n",
    "        self.device = device\n",
    "        self.load_model()\n",
    "        self.maxPersonNum = 2\n",
    "        self.kptNum = 17\n",
    "        self.maxFrameNum = 100\n",
    "        self.dim = 3\n",
    "        ## (batch, 100, 102)\n",
    "        self.p_interval = [0.95]\n",
    "        self.window_size = 64\n",
    "        self.thres = 64\n",
    "        self.clsNamePath = clsNamePath\n",
    "        self.load_clsNameTxt()\n",
    "    def load_clsNameTxt(self):\n",
    "        if os.path.exists(self.clsNamePath):\n",
    "            with open(self.clsNamePath,'r') as f:\n",
    "                self.clsList = [line[:-1] for line in f.readlines()]\n",
    "        else:\n",
    "            print(f'clsNamePath ({self.clsNamePath}) is not exists so skip')\n",
    "            self.clsList = None\n",
    "    def cropPreprocess(self,crop):\n",
    "        cropH,cropW,_ = crop.shape\n",
    "        assert cropH==256 and cropW==128\n",
    "        crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB).astype(float)/255\n",
    "        return torch.from_numpy(np.transpose(crop, (2, 0, 1)))#.unsqueeze(0)\n",
    "\n",
    "    def import_class(self,import_str):\n",
    "        mod_str, _sep, class_str = import_str.rpartition('.')\n",
    "        __import__(mod_str)\n",
    "        try:\n",
    "            return getattr(sys.modules[mod_str], class_str)\n",
    "        except AttributeError:\n",
    "            raise ImportError('Class %s cannot be found (%s)' % (class_str, traceback.format_exception(*sys.exc_info())))\n",
    "\n",
    "    def load_model(self):\n",
    "        print('[modelType] : ', self.modelType)\n",
    "        Model = self.import_class(self.modelType)\n",
    "        self.model = Model(**self.model_args)\n",
    "        weights = torch.load(self.ckptPath)\n",
    "        weights = OrderedDict([[k.split('module.')[-1], v.to(self.device)] for k, v in weights.items()])\n",
    "        keys = list(weights.keys())\n",
    "        self.model.load_state_dict(weights)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def normalization(self,data,frameShape=(1080,1920)):\n",
    "        height,width = frameShape\n",
    "        data[...,0]/=width\n",
    "        data[...,1]/=height\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def kptsPreprocess(self,kptSeqs,img):\n",
    "        imgSacle = img.shape[:-1]\n",
    "        kpts = [kptSeq[-1,:,:] for kptSeq in kptSeqs]\n",
    "        bboxs = [KptTracker.getBBOX_from_kpt(kpt[:,:,:].numpy()) for kpt in kpts]\n",
    "        # print('bbox len : ', len(bboxs))\n",
    "        crops = torch.stack([self.cropPreprocess(cv2.resize(img[int(y1):int(y2),int(x1):int(x2)],(128,256))) \n",
    "                             for x1,y1,x2,y2 in bboxs],dim=0)\n",
    "        \n",
    "        kptsN = [self.normalization(kptSeq.reshape(kptSeq.shape[1],51*kptSeq.shape[0]),frameShape=imgSacle) for kptSeq in kptSeqs]\n",
    "        kptsNN = np.zeros((len(kptsN), self.maxFrameNum, self.kptNum*self.maxPersonNum*self.dim), dtype=np.float32)\n",
    "        for dIdx,kptN in enumerate(kptsN):\n",
    "            if kptN.shape[-2] > self.maxFrameNum:\n",
    "                kptN = kptN[-self.maxFrameNum,:]\n",
    "            kptsNN[dIdx, :kptN.shape[0],:kptN.shape[-1]] = kptN\n",
    "        return kptsNN,crops,bboxs\n",
    "\n",
    "    def inference(self,Inputdata,Inputcrops):\n",
    "        inputdata = Inputdata.reshape((len(Inputcrops), self.maxFrameNum, 2, 17, 3)).transpose(0, 4, 1, 3, 2)\n",
    "        t_indexs = np.empty((len(inputdata),64))#,dtype=float\n",
    "        datas = np.empty((len(inputdata),3,64,17,2))#,dtype=float\n",
    "        t_indexs = np.empty((len(inputdata),64))#,dtype=float\n",
    "        valid_frame_nums = np.sum(np.sum(np.sum(inputdata, axis=-1), axis=-1) != 0, axis=-1)[...,0]\n",
    "\n",
    "        for i in range(len(inputdata)):\n",
    "            data,t_index = tools.valid_crop_uniform(inputdata[i], \n",
    "                                                    valid_frame_nums[i], \n",
    "                                                    self.p_interval, \n",
    "                                                    self.window_size, \n",
    "                                                    self.thres)\n",
    "            datas[i] = data\n",
    "            t_indexs[i] = t_index\n",
    "        datas = torch.from_numpy(datas).float().to(self.device)\n",
    "        t_indexs = torch.from_numpy(t_indexs).float().to(self.device)\n",
    "        Inputcrops = Inputcrops.float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(datas,t_indexs,Inputcrops)\n",
    "        preds = torch.nn.functional.softmax(preds, dim=0)\n",
    "        scores = torch.max(preds,dim=1).values.cpu()\n",
    "        prs = torch.argmax(preds,dim=1).cpu()\n",
    "\n",
    "        return prs,scores\n",
    "        \n",
    "    def drawBoundingBox(self,img, bboxs, actIs, actSs,fontScale=1,thickness=2):\n",
    "        for box,actI,actS in zip(bboxs,actIs,actSs):\n",
    "            x1,y1,x2,y2 = box[:4]\n",
    "            actI = self.clsList[actI] if self.clsList is not None else actI\n",
    "            actS = int(actS*100)\n",
    "            label = f'{actI}({actS}%)'\n",
    "            cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 6)\n",
    "            fontFace = cv2.FONT_HERSHEY_COMPLEX\n",
    "            labelSize = cv2.getTextSize(label, fontFace, fontScale, thickness)\n",
    "            _x1 = x1 # bottomleft x of text\n",
    "            _y1 = y1 # bottomleft y of text\n",
    "            _x2 = x1+labelSize[0][0] # topright x of text\n",
    "            _y2 = y1-labelSize[0][1] # topright y of text\n",
    "            cv2.rectangle(img, (_x1,_y1), (_x2,_y2), (0,255,0), cv2.FILLED) # text background\n",
    "            cv2.putText(img, label, (x1,y1), fontFace, fontScale, (0,0,0), thickness)\n",
    "        return img\n",
    "        \n",
    "        \n",
    "    def drawRecongizeResult(self, img, bboxs, actIs, actSs,fontScale=1,thickness=2):\n",
    "        return self.drawBoundingBox(img.copy(),bboxs,actIs, actSs,fontScale=1,thickness=2)\n",
    "        \n",
    "class VideoCaptureSave(object):\n",
    "    def __init__(self):\n",
    "        self.img_list = []\n",
    "        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4 codec\n",
    "        self.fps = 30  # Default frames per second\n",
    "        self.frame_width = None\n",
    "        self.frame_height = None\n",
    "        self.video_writer = None\n",
    "\n",
    "    def init_parameter(self, frame_width, frame_height, output_file, fps=30):\n",
    "        \"\"\"Initialize parameters and video capture.\"\"\"\n",
    "        self.fps = fps\n",
    "        \n",
    "        # Get frame dimensions\n",
    "        self.frame_width = int(frame_width)\n",
    "        self.frame_height = int(frame_height)\n",
    "        self.output_file = output_file\n",
    "        # Initialize video writer\n",
    "        self.video_writer = cv2.VideoWriter(output_file, self.fourcc, self.fps, (self.frame_width, self.frame_height))\n",
    "\n",
    "    def addImg2list(self,img):\n",
    "        #self.img_list.append(frame)\n",
    "        self.img_list.append(cv2.resize(img,(self.frame_width,self.frame_height)))\n",
    "\n",
    "    def saveImgList2mp4(self):\n",
    "        \"\"\"Save the images in the list to an MP4 file.\"\"\"\n",
    "        print(f'😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄 {len(self.img_list)}')\n",
    "        for img in self.img_list:\n",
    "            self.video_writer.write(cv2.resize(img,(self.frame_width,self.frame_height)))\n",
    "\n",
    "        # Release the video writer\n",
    "        self.video_writer.release()\n",
    "        print(f\"Video saved successfully.  ---> {self.output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b52efd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[modelType] :  model.SkateFormer.SkateFormerRGB_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k100/anaconda3/envs/motion/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "MotionRecognizeSkateFormerRGBer = MotionRecognizeSkateFormerRGB()\n",
    "KptTracker = KptTrack()\n",
    "PoseTracker = PoseTrackV8()\n",
    "VideoCaptureSaver = VideoCaptureSave()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b1712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = './samples/ntu_sample.avi'\n",
    "# videoPath = './samples/vlc-record-2024-04-24-15h03m46s-0x645A1775_20230509183809_20230509185044.avi-.mp4'\n",
    "cap = cv2.VideoCapture(videoPath)\n",
    "\n",
    "outs = []\n",
    "count = 0\n",
    "while True:\n",
    "    ret,img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    out = PoseTracker.poseTrack(img)[0]\n",
    "    outs.append(out)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797fa737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:02<00:00, 33.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄😄 72\n",
      "Video saved successfully.  ---> out.mp4\n"
     ]
    }
   ],
   "source": [
    "VideoCaptureSaver.init_parameter(960, 540, 'out.mp4')\n",
    "for out in tqdm(outs):\n",
    "    img = out.orig_img\n",
    "    imgH,imgW,_ = img.shape\n",
    "    trackIDs = out.boxes.id\n",
    "    trackIDs = trackIDs.cpu().numpy() if trackIDs is not None else np.array([])\n",
    "    keypoints = out.keypoints.data.cpu() if len(trackIDs) else torch.tensor([])\n",
    "    # keypoints[...,0]*=(1920/imgW)\n",
    "    # keypoints[...,1]*=(1080/imgH)\n",
    "    trackIDs,kptSeqs = KptTracker.tracking(trackIDs,keypoints)\n",
    "    kptsFeatures,crops,bboxes = MotionRecognizeSkateFormerRGBer.kptsPreprocess([kptSeq.unsqueeze(0) \n",
    "                                                                         for kptSeq in kptSeqs],img)\n",
    "    preds,scores = MotionRecognizeSkateFormerRGBer.inference(kptsFeatures,crops)\n",
    "    img_plot = MotionRecognizeSkateFormerRGBer.drawRecongizeResult(img,bboxes,preds,scores,\n",
    "                                                                   fontScale=1*(imgW/1920),thickness=int(2*(imgW/1920)))\n",
    "    VideoCaptureSaver.addImg2list(img_plot)\n",
    "VideoCaptureSaver.saveImgList2mp4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350d4b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = PoseTracker.poseTrack('sample.png')[0]\n",
    "h,w,_ = result.orig_img.shape\n",
    "result.orig_img = np.ones((h,w,3),dtype='uint8')*255\n",
    "cv2.imwrite('sample_pose.png',result.plot(labels=False,boxes=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011c04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
